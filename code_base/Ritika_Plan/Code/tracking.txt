date and time = 14/01/2023 00:39:48
DataParallel(
  (module): Model(
    (seq_layer1): Sequential(
      (0): Conv1d(3, 64, kernel_size=(128,), stride=(16,))
      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)
      (4): Dropout(p=0.3, inplace=False)
      (5): Conv1d(64, 128, kernel_size=(8,), stride=(1,))
      (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): ReLU()
      (8): Conv1d(128, 128, kernel_size=(8,), stride=(1,))
      (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): ReLU()
      (11): Conv1d(128, 128, kernel_size=(8,), stride=(1,))
      (12): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): ReLU()
      (14): Flatten(start_dim=1, end_dim=-1)
      (15): Linear(in_features=4864, out_features=1024, bias=True)
      (16): ReLU()
      (17): Dropout(p=0.3, inplace=False)
      (18): Linear(in_features=1024, out_features=512, bias=True)
      (19): ReLU()
      (20): Dropout(p=0.3, inplace=False)
      (21): Linear(in_features=512, out_features=64, bias=True)
      (22): ReLU()
      (23): Dropout(p=0.3, inplace=False)
      (24): Linear(in_features=64, out_features=8, bias=True)
      (25): Softmax(dim=-1)
    )
  )
)
	 epoch:0, T.acc:52.856, V.acc:11.567
		 T.loss:1.74468, V.loss:2.15834
	 epoch:5, T.acc:52.111, V.acc:39.703
		 T.loss:1.75289, V.loss:1.87698
	 epoch:10, T.acc:51.112, V.acc:46.822
		 T.loss:1.76288, V.loss:1.80576
	 epoch:15, T.acc:53.002, V.acc:37.106
		 T.loss:1.74398, V.loss:1.90295
	 epoch:20, T.acc:50.726, V.acc:47.539
		 T.loss:1.76675, V.loss:1.79862
	 epoch:25, T.acc:51.962, V.acc:56.012
		 T.loss:1.75439, V.loss:1.71389
	 epoch:30, T.acc:50.993, V.acc:45.781
		 T.loss:1.76407, V.loss:1.81619
	 epoch:35, T.acc:50.836, V.acc:49.858
		 T.loss:1.76565, V.loss:1.77543
	 epoch:40, T.acc:52.292, V.acc:44.220
		 T.loss:1.75109, V.loss:1.83181
	 epoch:45, T.acc:54.224, V.acc:49.413
		 T.loss:1.73178, V.loss:1.77989
	 epoch:50, T.acc:50.711, V.acc:37.204
		 T.loss:1.76689, V.loss:1.90197
	 epoch:55, T.acc:49.932, V.acc:51.732
		 T.loss:1.77469, V.loss:1.75669
	 epoch:60, T.acc:45.380, V.acc:41.062
		 T.loss:1.82021, V.loss:1.86339
	 epoch:65, T.acc:49.425, V.acc:51.316
		 T.loss:1.77976, V.loss:1.76085
	 epoch:70, T.acc:55.071, V.acc:57.810
		 T.loss:1.72330, V.loss:1.69590
	 epoch:75, T.acc:53.998, V.acc:53.901
		 T.loss:1.73403, V.loss:1.73500
	 epoch:80, T.acc:54.894, V.acc:54.132
		 T.loss:1.72506, V.loss:1.73269
	 epoch:85, T.acc:50.499, V.acc:50.252
		 T.loss:1.76902, V.loss:1.77149
	 epoch:90, T.acc:52.772, V.acc:37.204
		 T.loss:1.74629, V.loss:1.90197
	 epoch:95, T.acc:52.140, V.acc:57.440
		 T.loss:1.75260, V.loss:1.69961
Early stopping :(
	 epoch:99, T.acc:54.701, V.acc:54.942
		 T.loss:1.72700, V.loss:1.72459
Test_fold: Tloss: 2.08024, Tacc: 11.567



date and time = 14/01/2023 10:27:59
DataParallel(
  (module): Model(
    (seq_layer1): Sequential(
      (0): Conv1d(3, 64, kernel_size=(128,), stride=(32,))
      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)
      (4): Dropout(p=0.3, inplace=False)
      (5): Conv1d(64, 128, kernel_size=(8,), stride=(1,))
      (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): ReLU()
      (8): Conv1d(128, 128, kernel_size=(8,), stride=(1,))
      (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): ReLU()
      (11): Conv1d(128, 128, kernel_size=(8,), stride=(1,))
      (12): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): ReLU()
      (14): Flatten(start_dim=1, end_dim=-1)
      (15): Linear(in_features=1024, out_features=512, bias=True)
      (16): ReLU()
      (17): Dropout(p=0.3, inplace=False)
      (18): Linear(in_features=512, out_features=64, bias=True)
      (19): ReLU()
      (20): Dropout(p=0.3, inplace=False)
      (21): Linear(in_features=64, out_features=8, bias=True)
      (22): Softmax(dim=-1)
    )
  )
)
	 epoch:0, T.acc:55.816, V.acc:11.532
		 T.loss:1.71620, V.loss:2.15869
	 epoch:5, T.acc:57.931, V.acc:38.876
		 T.loss:1.69446, V.loss:1.88487
	 epoch:10, T.acc:57.860, V.acc:11.567
		 T.loss:1.69533, V.loss:2.15834
	 epoch:15, T.acc:58.102, V.acc:37.742
		 T.loss:1.69280, V.loss:1.89653
	 epoch:20, T.acc:57.968, V.acc:39.153
		 T.loss:1.69422, V.loss:1.88198
	 epoch:25, T.acc:57.970, V.acc:11.567
		 T.loss:1.69427, V.loss:2.15834
	 epoch:30, T.acc:58.216, V.acc:38.656
		 T.loss:1.69182, V.loss:1.88741
	 epoch:35, T.acc:58.083, V.acc:53.525
		 T.loss:1.69318, V.loss:1.73882
	 epoch:40, T.acc:57.617, V.acc:38.054
		 T.loss:1.69780, V.loss:1.89346
	 epoch:45, T.acc:57.582, V.acc:11.567
		 T.loss:1.69818, V.loss:2.15834
	 epoch:50, T.acc:58.201, V.acc:38.748
		 T.loss:1.69197, V.loss:1.88649
	 epoch:55, T.acc:57.604, V.acc:44.861
		 T.loss:1.69794, V.loss:1.82541
	 epoch:60, T.acc:57.789, V.acc:57.203
		 T.loss:1.69608, V.loss:1.70199
	 epoch:65, T.acc:57.084, V.acc:41.091
		 T.loss:1.70315, V.loss:1.86311
	 epoch:70, T.acc:58.266, V.acc:53.976
		 T.loss:1.69133, V.loss:1.73423
	 epoch:75, T.acc:57.286, V.acc:38.662
		 T.loss:1.70114, V.loss:1.88735
	 epoch:80, T.acc:57.685, V.acc:24.967
		 T.loss:1.69713, V.loss:2.02436
	 epoch:85, T.acc:57.801, V.acc:39.500
		 T.loss:1.69600, V.loss:1.87889
	 epoch:90, T.acc:56.397, V.acc:57.157
		 T.loss:1.71004, V.loss:1.70244
	 epoch:95, T.acc:56.057, V.acc:45.908
		 T.loss:1.71341, V.loss:1.81492
Early stopping :(
	 epoch:99, T.acc:57.718, V.acc:47.690
		 T.loss:1.69682, V.loss:1.79713
Test_fold: Tloss: 2.07889, Tacc: 11.567



date and time = 14/01/2023 14:23:36
picked channels = ['C4-A1', 'ROC-LOC', 'EMG1-EMG2']
DataParallel(
  (module): Model(
    (seq_layer1): Sequential(
      (0): Conv1d(3, 64, kernel_size=(128,), stride=(16,))
      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)
      (4): Dropout(p=0.3, inplace=False)
      (5): Conv1d(64, 128, kernel_size=(8,), stride=(1,))
      (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): ReLU()
      (8): Conv1d(128, 128, kernel_size=(8,), stride=(1,))
      (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): ReLU()
      (11): Conv1d(128, 128, kernel_size=(8,), stride=(1,))
      (12): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): ReLU()
      (14): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
    )
    (seq_layer2): Sequential(
      (0): Conv1d(3, 64, kernel_size=(1024,), stride=(128,))
      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
      (4): Dropout(p=0.3, inplace=False)
      (5): Conv1d(64, 128, kernel_size=(6,), stride=(1,))
      (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): ReLU()
      (8): Conv1d(128, 128, kernel_size=(6,), stride=(1,))
      (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): ReLU()
      (11): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (fullyConnected): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
      (1): Linear(in_features=1280, out_features=1024, bias=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
      (4): Linear(in_features=1024, out_features=512, bias=True)
      (5): ReLU()
      (6): Dropout(p=0.3, inplace=False)
      (7): Linear(in_features=512, out_features=64, bias=True)
      (8): ReLU()
      (9): Dropout(p=0.3, inplace=False)
      (10): Linear(in_features=64, out_features=7, bias=True)
      (11): Softmax(dim=-1)
    )
  )
)
	 epoch:0, T.acc:37.430, V.acc:37.737
		 T.loss:1.79101, V.loss:1.78805
	 epoch:5, T.acc:37.734, V.acc:37.737
		 T.loss:1.78809, V.loss:1.78805
	 epoch:10, T.acc:37.734, V.acc:37.737
		 T.loss:1.78809, V.loss:1.78805
	 epoch:15, T.acc:37.734, V.acc:37.737
		 T.loss:1.78809, V.loss:1.78805
	 epoch:20, T.acc:37.734, V.acc:37.737
		 T.loss:1.78809, V.loss:1.78805




date and time = 14/01/2023 14:57:22
picked channels = ['C4-A1', 'ROC-LOC', 'EMG1-EMG2']
DataParallel(
  (module): Model(
    (seq_layer1): Sequential(
      (0): Conv1d(3, 64, kernel_size=(128,), stride=(16,))
      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)
      (4): Dropout(p=0.3, inplace=False)
      (5): Conv1d(64, 128, kernel_size=(8,), stride=(1,))
      (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): ReLU()
      (8): Conv1d(128, 128, kernel_size=(8,), stride=(1,))
      (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): ReLU()
      (11): Conv1d(128, 128, kernel_size=(8,), stride=(1,))
      (12): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): ReLU()
      (14): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
    )
    (seq_layer2): Sequential(
      (0): Conv1d(3, 64, kernel_size=(1024,), stride=(128,))
      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
      (4): Dropout(p=0.3, inplace=False)
      (5): Conv1d(64, 128, kernel_size=(6,), stride=(1,))
      (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): ReLU()
      (8): Conv1d(128, 128, kernel_size=(6,), stride=(1,))
      (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): ReLU()
      (11): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (fullyConnected): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
      (1): Linear(in_features=1280, out_features=1024, bias=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
      (4): Linear(in_features=1024, out_features=512, bias=True)
      (5): ReLU()
      (6): Dropout(p=0.3, inplace=False)
      (7): Linear(in_features=512, out_features=64, bias=True)
      (8): ReLU()
      (9): Dropout(p=0.3, inplace=False)
      (10): Linear(in_features=64, out_features=3, bias=True)
      (11): Softmax(dim=-1)
    )
  )
)
	 epoch:0, T.acc:76.991, V.acc:34.329
		 T.loss:0.77928, V.loss:1.19919
	 epoch:5, T.acc:82.356, V.acc:54.066
		 T.loss:0.72709, V.loss:1.00731
	 epoch:10, T.acc:85.330, V.acc:14.511
		 T.loss:0.69656, V.loss:1.28128
	 epoch:15, T.acc:87.522, V.acc:16.791
		 T.loss:0.67454, V.loss:1.32061
	 epoch:20, T.acc:87.152, V.acc:52.207
		 T.loss:0.67878, V.loss:1.02937
	 epoch:25, T.acc:87.789, V.acc:52.207
		 T.loss:0.67166, V.loss:1.02937
	 epoch:30, T.acc:88.575, V.acc:14.657
		 T.loss:0.66422, V.loss:1.40397
	 epoch:35, T.acc:88.696, V.acc:52.207
		 T.loss:0.66314, V.loss:1.02937
	 epoch:40, T.acc:87.976, V.acc:52.207
		 T.loss:0.67101, V.loss:1.02937
	 epoch:45, T.acc:88.128, V.acc:52.224
		 T.loss:0.66932, V.loss:1.02921
	 epoch:50, T.acc:88.727, V.acc:37.510
		 T.loss:0.66252, V.loss:1.17498
	 epoch:55, T.acc:89.510, V.acc:33.306
		 T.loss:0.65563, V.loss:1.21838
	 epoch:60, T.acc:89.557, V.acc:24.119
		 T.loss:0.65518, V.loss:1.30963
	 epoch:65, T.acc:89.043, V.acc:52.207
		 T.loss:0.66011, V.loss:1.02937
	 epoch:70, T.acc:89.045, V.acc:21.904
		 T.loss:0.65984, V.loss:1.13715
	 epoch:75, T.acc:88.670, V.acc:15.136
		 T.loss:0.66368, V.loss:1.39697
	 epoch:80, T.acc:88.315, V.acc:52.207
		 T.loss:0.66736, V.loss:1.02937
	 epoch:85, T.acc:89.234, V.acc:34.037
		 T.loss:0.65772, V.loss:1.21095
	 epoch:90, T.acc:89.877, V.acc:33.339
		 T.loss:0.65179, V.loss:1.21806



date and time = 14/01/2023 15:31:06
picked channels = ['P4-O2', 'ROC-LOC', 'EMG1-EMG2']
classes : dict_values(['Nocturnal frontal lobe epilepsy', 'Periodic leg movements', 'REM behavior disorder'])
DataParallel(
  (module): Model(
    (seq_layer1): Sequential(
      (0): Conv1d(3, 64, kernel_size=(128,), stride=(16,))
      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)
      (4): Dropout(p=0.5, inplace=False)
      (5): Conv1d(64, 128, kernel_size=(8,), stride=(1,))
      (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): ReLU()
      (8): Conv1d(128, 128, kernel_size=(8,), stride=(1,))
      (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): ReLU()
      (11): Conv1d(128, 128, kernel_size=(8,), stride=(1,))
      (12): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (13): ReLU()
      (14): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
    )
    (seq_layer2): Sequential(
      (0): Conv1d(3, 64, kernel_size=(1024,), stride=(128,))
      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
      (4): Dropout(p=0.5, inplace=False)
      (5): Conv1d(64, 128, kernel_size=(6,), stride=(1,))
      (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): ReLU()
      (8): Conv1d(128, 128, kernel_size=(6,), stride=(1,))
      (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (10): ReLU()
      (11): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (fullyConnected): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
      (1): Linear(in_features=1280, out_features=512, bias=True)
      (2): ReLU()
      (3): Dropout(p=0.5, inplace=False)
      (4): Linear(in_features=512, out_features=64, bias=True)
      (5): ReLU()
      (6): Dropout(p=0.5, inplace=False)
      (7): Linear(in_features=64, out_features=3, bias=True)
      (8): Softmax(dim=-1)
    )
  )
)
	 epoch:0, T.acc:77.628, V.acc:52.207
			T.loss:0.77464, V.loss:1.02937
	 epoch:5, T.acc:81.816, V.acc:39.661
			T.loss:0.73274, V.loss:1.04873
	 epoch:10, T.acc:82.346, V.acc:52.207
			T.loss:0.72711, V.loss:1.02937
	 epoch:15, T.acc:83.309, V.acc:14.535
			T.loss:0.71723, V.loss:1.37516
	 epoch:20, T.acc:84.131, V.acc:14.486
			T.loss:0.70936, V.loss:1.40653
	 epoch:25, T.acc:84.606, V.acc:16.053
			T.loss:0.70443, V.loss:1.39079
Early stopping :(
	 epoch:29, T.acc:84.963, V.acc:37.762
			T.loss:0.70097, V.loss:1.15692
Test_fold: Tloss: 1.10018, Tacc: 33.307
